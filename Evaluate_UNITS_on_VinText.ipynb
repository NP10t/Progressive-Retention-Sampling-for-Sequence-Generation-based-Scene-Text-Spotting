{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rImLPwTq_-1v",
        "outputId": "04854745-7be5-4616-f0a7-faa1ceffd94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.9.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: Polygon3 in /usr/local/lib/python3.10/dist-packages (3.0.9.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install Levenshtein\n",
        "!pip install gdown\n",
        "!pip install Polygon3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvuBTcg7_-12"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "P_hrFs1c_-15",
        "outputId": "b3d2d6a4-988e-4e26-f9da-eb768698be53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ibgN01mg4wFPMq0cwYFqp0h3gRnmD_EZ\n",
            "To: /content/vintext_label.zip\n",
            "100%|██████████| 302k/302k [00:00<00:00, 11.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vintext_label.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "url = 'https://drive.google.com/uc?id=1ibgN01mg4wFPMq0cwYFqp0h3gRnmD_EZ'\n",
        "gdown.download(url, 'vintext_label.zip', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9B81MzRUBdj",
        "outputId": "e67e5e38-8609-4d05-bfc5-c08c1ed28a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @converter"
      ],
      "metadata": {
        "id": "6PeJ4UFzS6EU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaEoRkRqo2mQ"
      },
      "outputs": [],
      "source": [
        "dictionary = \"aàáạảãâầấậẩẫăằắặẳẵAÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪeèéẹẻẽêềếệểễEÈÉẸẺẼÊỀẾỆỂỄoòóọỏõôồốộổỗơờớợởỡOÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠiìíịỉĩIÌÍỊỈĨuùúụủũưừứựửữƯỪỨỰỬỮUÙÚỤỦŨyỳýỵỷỹYỲÝỴỶỸ\"\n",
        "\n",
        "\n",
        "def make_groups():\n",
        "    groups = []\n",
        "    i = 0\n",
        "    while i < len(dictionary) - 5:\n",
        "        group = [c for c in dictionary[i : i + 6]]\n",
        "        i += 6\n",
        "        groups.append(group)\n",
        "    return groups\n",
        "\n",
        "\n",
        "groups = make_groups()\n",
        "\n",
        "TONES = [\"\", \"ˋ\", \"ˊ\", \"⸱\", \"ˀ\", \"˜\"]\n",
        "SOURCES = [\"ă\", \"â\", \"Ă\", \"Â\", \"ê\", \"Ê\", \"ô\", \"ơ\", \"Ô\", \"Ơ\", \"ư\", \"Ư\", \"Đ\", \"đ\"]\n",
        "TARGETS = [\"aˇ\", \"aˆ\", \"Aˇ\", \"Aˆ\", \"eˆ\", \"Eˆ\", \"oˆ\", \"o˒\", \"Oˆ\", \"O˒\", \"u˒\", \"U˒\", \"D^\", \"d^\"]\n",
        "\n",
        "\n",
        "def parse_tone(word):\n",
        "    res = \"\"\n",
        "    tone = \"\"\n",
        "    for char in word:\n",
        "        if char in dictionary:\n",
        "            for group in groups:\n",
        "                if char in group:\n",
        "                    if tone == \"\":\n",
        "                        tone = TONES[group.index(char)]\n",
        "                    res += group[0]\n",
        "        else:\n",
        "            res += char\n",
        "    res += tone\n",
        "    return res\n",
        "\n",
        "\n",
        "def full_parse(word):\n",
        "    word = parse_tone(word)\n",
        "    res = \"\"\n",
        "    for char in word:\n",
        "        if char in SOURCES:\n",
        "            res += TARGETS[SOURCES.index(char)]\n",
        "        else:\n",
        "            res += char\n",
        "    return res\n",
        "\n",
        "\n",
        "def correct_tone_position(word):\n",
        "    word = word[:-1]\n",
        "    first_ord_char = \"\"\n",
        "    second_order_char = \"\"\n",
        "    for char in word:\n",
        "        for group in groups:\n",
        "            if char in group:\n",
        "                second_order_char = first_ord_char\n",
        "                first_ord_char = group[0]\n",
        "    if len(word) >= 1 and word[-1] == first_ord_char and second_order_char != \"\":\n",
        "        pair_chars = [\"qu\", \"Qu\", \"qU\", \"QU\", \"gi\", \"Gi\", \"gI\", \"GI\"]\n",
        "        for pair in pair_chars:\n",
        "            if pair in word and second_order_char in [\"u\", \"U\", \"i\", \"I\"]:\n",
        "                return first_ord_char\n",
        "        return second_order_char\n",
        "    return first_ord_char\n",
        "\n",
        "\n",
        "def decoder(recognition):\n",
        "    for char in TARGETS:\n",
        "        recognition = recognition.replace(char, SOURCES[TARGETS.index(char)])\n",
        "    replace_char = correct_tone_position(recognition)\n",
        "    if recognition[-1] in TONES:\n",
        "        tone = recognition[-1]\n",
        "        recognition = recognition[:-1]\n",
        "        for group in groups:\n",
        "            if replace_char in group:\n",
        "                recognition = recognition.replace(replace_char, group[TONES.index(tone)])\n",
        "    return recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Eval"
      ],
      "metadata": {
        "id": "xiuSwZxvTE3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLx4Gb1mTI02",
        "outputId": "5bd19708-6875-41ee-8dad-3e0631d67d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã hoàn thành việc sao chép và lọc các tệp.\n",
            "res_output_filtered_thrs_0.0.zip\n"
          ]
        }
      ],
      "source": [
        "CONFIDENT = 0.0\n",
        "\n",
        "num_pts = 8\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "source_dir = r\"res_output\"\n",
        "if os.path.exists(source_dir):\n",
        "    shutil.rmtree(source_dir)\n",
        "\n",
        "!tar -xzf res_output_final6000.tar.gz\n",
        "# !tar -xzf res_output_effortless.tar.gz\n",
        "\n",
        "destination_dir = source_dir + \"_filtered_thrs_\" + str(CONFIDENT)\n",
        "\n",
        "if os.path.exists(destination_dir):\n",
        "    shutil.rmtree(destination_dir)\n",
        "os.makedirs(destination_dir)\n",
        "\n",
        "for filename in os.listdir(source_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        numbers = re.findall(r'\\d+', filename)\n",
        "        source_file = os.path.join(source_dir, filename)\n",
        "        destination_file = os.path.join(destination_dir, '000' + str(int(numbers[0])) + '.txt')\n",
        "\n",
        "        with open(source_file, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        filtered_lines = []\n",
        "        # for line in lines:\n",
        "        #     parts = line.strip().split(',', num_pts+1)\n",
        "        #     if len(parts) == 10:\n",
        "        #         coordinates = ','.join(parts[:num_pts])\n",
        "        #         confidence = float(parts[num_pts])\n",
        "        #         text = parts[num_pts+1]\n",
        "\n",
        "        #         if confidence > CONFIDENT:\n",
        "        #             filtered_line = f\"{coordinates},####{decoder(text)}\\n\"\n",
        "        #             # filtered_line = f\"{coordinates},####{text}\\n\"\n",
        "        #             filtered_lines.append(filtered_line)\n",
        "\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(',', num_pts+1)\n",
        "            if len(parts) == 10:\n",
        "                coordinates = ','.join(parts[:num_pts])\n",
        "                confidence = float(parts[num_pts][4:])\n",
        "                text = parts[num_pts+1][4:]\n",
        "\n",
        "                if confidence > CONFIDENT:\n",
        "                    filtered_line = f\"{coordinates},####{text}\\n\"\n",
        "                    filtered_lines.append(filtered_line)\n",
        "\n",
        "            with open(destination_file, 'w', encoding='utf-8') as file:\n",
        "                file.writelines(filtered_lines)\n",
        "\n",
        "shutil.make_archive(destination_dir, 'zip', destination_dir)\n",
        "\n",
        "print(\"Đã hoàn thành việc sao chép và lọc các tệp.\")\n",
        "print(destination_dir + \".zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaKJy_7r_-16",
        "outputId": "d9d91606-adbf-4424-b34e-5755d226cc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission file path:  ./res_output_filtered_thrs_0.0.zip\n",
            "gt file path:  ./vintext_label.zip\n",
            "X:  6554\n",
            "Y:  1784\n",
            "X+Y:  8338\n",
            "matchedSum:  6554\n",
            "numGlobalCareDet:  12557\n",
            "max_fail:  7\n",
            "name_fail:  0001501\n",
            "-------\n",
            "numGlobalCareGt:  9769\n",
            "Calculated!\n",
            "\"E2E_RESULTS: precision: 0.5219399538106235, recall: 0.6708977377418365, hmean: 0.5871181582012003\"\n",
            "\"DETECTION_ONLY_RESULTS: precision: 0.6635427155139029, recall: 0.8493952012690859, hmean: 0.7450537026568683\"\n",
            "./res_output_filtered_thrs_0.0.zip\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import importlib\n",
        "import sys\n",
        "from collections import namedtuple\n",
        "import Levenshtein as lstn\n",
        "import codecs\n",
        "from shapely.geometry import *\n",
        "\n",
        "\n",
        "def evaluate_with_official_code(result_path, gt_path):\n",
        "    _word_spotting = True\n",
        "    return text_eval_main(\n",
        "        det_file=result_path, gt_file=gt_path, is_word_spotting=_word_spotting\n",
        "    )\n",
        "\n",
        "\n",
        "def text_eval_main(det_file, gt_file, is_word_spotting):\n",
        "    global WORD_SPOTTING\n",
        "    WORD_SPOTTING = is_word_spotting\n",
        "    return main_evaluation(\n",
        "        None,\n",
        "        det_file,\n",
        "        gt_file,\n",
        "        default_evaluation_params,\n",
        "        validate_data,\n",
        "        evaluate_method,\n",
        "    )\n",
        "\n",
        "\n",
        "def main_evaluation(\n",
        "    p,\n",
        "    det_file,\n",
        "    gt_file,\n",
        "    default_evaluation_params_fn,\n",
        "    validate_data_fn,\n",
        "    evaluate_method_fn,\n",
        "    show_result=True,\n",
        "    per_sample=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    This process validates a method, evaluates it and if it succed generates a ZIP file with a JSON entry for each sample.\n",
        "    Params:\n",
        "    p: Dictionary of parmeters with the GT/submission locations. If None is passed, the parameters send by the system are used.\n",
        "    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n",
        "    validate_data_fn: points to a method that validates the corrct format of the submission\n",
        "    evaluate_method_fn: points to a function that evaluated the submission and return a Dictionary with the results\n",
        "    \"\"\"\n",
        "\n",
        "    # if (p == None):\n",
        "    #     p = dict([s[1:].split('=') for s in sys.argv[1:]])\n",
        "    #     if(len(sys.argv)<3):\n",
        "    #         print_help()\n",
        "    p = {}\n",
        "    p[\"g\"] = gt_file  #'tttgt.zip'\n",
        "    p[\"s\"] = det_file  #'det.zip'\n",
        "\n",
        "    evalParams = default_evaluation_params_fn()\n",
        "    if \"p\" in p.keys():\n",
        "        evalParams.update(\n",
        "            p[\"p\"] if isinstance(p[\"p\"], dict) else json.loads(p[\"p\"][1:-1])\n",
        "        )\n",
        "\n",
        "    resDict = {\"calculated\": True, \"Message\": \"\", \"method\": \"{}\", \"per_sample\": \"{}\"}\n",
        "    # try:\n",
        "    validate_data_fn(p[\"g\"], p[\"s\"], evalParams)\n",
        "    evalData = evaluate_method_fn(p[\"g\"], p[\"s\"], evalParams)\n",
        "    resDict.update(evalData)\n",
        "\n",
        "    # except Exception as e:\n",
        "    # resDict['Message']= str(e)\n",
        "    # resDict['calculated']=False\n",
        "\n",
        "    if \"o\" in p:\n",
        "        if not os.path.exists(p[\"o\"]):\n",
        "            os.makedirs(p[\"o\"])\n",
        "\n",
        "        resultsOutputname = p[\"o\"] + \"/results.zip\"\n",
        "        outZip = zipfile.ZipFile(resultsOutputname, mode=\"w\", allowZip64=True)\n",
        "\n",
        "        del resDict[\"per_sample\"]\n",
        "        if \"output_items\" in resDict.keys():\n",
        "            del resDict[\"output_items\"]\n",
        "\n",
        "        outZip.writestr(\"method.json\", json.dumps(resDict))\n",
        "\n",
        "    if not resDict[\"calculated\"]:\n",
        "        if show_result:\n",
        "            sys.stderr.write(\"Error!\\n\" + resDict[\"Message\"] + \"\\n\\n\")\n",
        "        if \"o\" in p:\n",
        "            outZip.close()\n",
        "        return resDict\n",
        "\n",
        "    if \"o\" in p:\n",
        "        if per_sample == True:\n",
        "            for k, v in evalData[\"per_sample\"].items():\n",
        "                outZip.writestr(k + \".json\", json.dumps(v))\n",
        "\n",
        "            if \"output_items\" in evalData.keys():\n",
        "                for k, v in evalData[\"output_items\"].items():\n",
        "                    outZip.writestr(k, v)\n",
        "\n",
        "        outZip.close()\n",
        "\n",
        "    if show_result:\n",
        "        sys.stdout.write(\"Calculated!\")\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        sys.stdout.write(json.dumps(resDict[\"e2e_method\"]))\n",
        "        sys.stdout.write(\"\\n\")\n",
        "        sys.stdout.write(json.dumps(resDict[\"det_only_method\"]))\n",
        "        sys.stdout.write(\"\\n\")\n",
        "\n",
        "    return resDict\n",
        "\n",
        "\n",
        "def default_evaluation_params():\n",
        "    \"\"\"\n",
        "    default_evaluation_params: Default parameters to use for the validation and evaluation.\n",
        "    \"\"\"\n",
        "    global WORD_SPOTTING\n",
        "    return {\n",
        "        \"IOU_CONSTRAINT\": 0.5,\n",
        "        \"AREA_PRECISION_CONSTRAINT\": 0.5,\n",
        "        \"WORD_SPOTTING\": WORD_SPOTTING,\n",
        "        \"MIN_LENGTH_CARE_WORD\": 0,\n",
        "        \"GT_SAMPLE_NAME_2_ID\": \"([0-9]+).txt\",\n",
        "        \"DET_SAMPLE_NAME_2_ID\": \"([0-9]+).txt\",\n",
        "        \"LTRB\": False,  # LTRB:2points(left,top,right,bottom) or 4 points(x1,y1,x2,y2,x3,y3,x4,y4)\n",
        "        \"CRLF\": False,  # Lines are delimited by Windows CRLF format\n",
        "        \"CONFIDENCES\": False,  # Detections must include confidence value. MAP and MAR will be calculated,\n",
        "        \"SPECIAL_CHARACTERS\": str(\"!?.:,*\\\"()·[]/'\"),\n",
        "        \"ONLY_REMOVE_FIRST_LAST_CHARACTER\": True,\n",
        "    }\n",
        "\n",
        "\n",
        "def validate_data(gtFilePath, submFilePath, evaluationParams):\n",
        "    \"\"\"\n",
        "    Method validate_data: validates that all files in the results folder are correct (have the correct name contents).\n",
        "                            Validates also that there are no missing files in the folder.\n",
        "                            If some error detected, the method raises the error\n",
        "    \"\"\"\n",
        "    gt = load_zip_file(gtFilePath, evaluationParams[\"GT_SAMPLE_NAME_2_ID\"])\n",
        "\n",
        "    subm = load_zip_file(submFilePath, evaluationParams[\"DET_SAMPLE_NAME_2_ID\"], True)\n",
        "\n",
        "\n",
        "def evaluation_imports():\n",
        "    \"\"\"\n",
        "    evaluation_imports: Dictionary ( key = module name , value = alias  )  with python modules used in the evaluation.\n",
        "    \"\"\"\n",
        "    return {\"Polygon\": \"plg\", \"numpy\": \"np\"}\n",
        "\n",
        "\n",
        "def evaluate_method(gtFilePath, submFilePath, evaluationParams):\n",
        "    \"\"\"\n",
        "    Method evaluate_method: evaluate method and returns the results\n",
        "        Results. Dictionary with the following values:\n",
        "        - method (required)  Global method metrics. Ex: { 'Precision':0.8,'Recall':0.9 }\n",
        "        - samples (optional) Per sample metrics. Ex: {'sample1' : { 'Precision':0.8,'Recall':0.9 } , 'sample2' : { 'Precision':0.8,'Recall':0.9 }\n",
        "    \"\"\"\n",
        "    for module, alias in evaluation_imports().items():\n",
        "        globals()[alias] = importlib.import_module(module)\n",
        "\n",
        "    def polygon_from_points(points):\n",
        "        \"\"\"\n",
        "        Returns a Polygon object to use with the Polygon2 class from a list of 8 points: x1,y1,x2,y2,x3,y3,x4,y4\n",
        "        \"\"\"\n",
        "        num_points = len(points)\n",
        "        # resBoxes=np.empty([1,num_points],dtype='int32')\n",
        "        resBoxes = np.empty([1, num_points], dtype=\"float32\")\n",
        "        for inp in range(0, num_points, 2):\n",
        "            resBoxes[0, int(inp / 2)] = float(points[int(inp)])\n",
        "            resBoxes[0, int(inp / 2 + num_points / 2)] = float(points[int(inp + 1)])\n",
        "        pointMat = resBoxes[0].reshape([2, int(num_points / 2)]).T\n",
        "        return plg.Polygon(pointMat)\n",
        "\n",
        "    def rectangle_to_polygon(rect):\n",
        "        resBoxes = np.empty([1, 8], dtype=\"int32\")\n",
        "        resBoxes[0, 0] = int(rect.xmin)\n",
        "        resBoxes[0, 4] = int(rect.ymax)\n",
        "        resBoxes[0, 1] = int(rect.xmin)\n",
        "        resBoxes[0, 5] = int(rect.ymin)\n",
        "        resBoxes[0, 2] = int(rect.xmax)\n",
        "        resBoxes[0, 6] = int(rect.ymin)\n",
        "        resBoxes[0, 3] = int(rect.xmax)\n",
        "        resBoxes[0, 7] = int(rect.ymax)\n",
        "\n",
        "        pointMat = resBoxes[0].reshape([2, 4]).T\n",
        "\n",
        "        return plg.Polygon(pointMat)\n",
        "\n",
        "    def rectangle_to_points(rect):\n",
        "        points = [\n",
        "            int(rect.xmin),\n",
        "            int(rect.ymax),\n",
        "            int(rect.xmax),\n",
        "            int(rect.ymax),\n",
        "            int(rect.xmax),\n",
        "            int(rect.ymin),\n",
        "            int(rect.xmin),\n",
        "            int(rect.ymin),\n",
        "        ]\n",
        "        return points\n",
        "\n",
        "    def get_union(pD, pG):\n",
        "        areaA = pD.area()\n",
        "        areaB = pG.area()\n",
        "        return areaA + areaB - get_intersection(pD, pG)\n",
        "\n",
        "    def get_intersection_over_union(pD, pG):\n",
        "        # print(\"i: \", get_intersection(pD, pG))\n",
        "        # print(\"u: \", get_union(pD, pG))\n",
        "        try:\n",
        "            return get_intersection(pD, pG) / get_union(pD, pG)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def get_intersection(pD, pG):\n",
        "        # print(\"pD\", pD)\n",
        "        # print(\"pG\", pG)\n",
        "        pInt = pD & pG\n",
        "        if len(pInt) == 0:\n",
        "            return 0\n",
        "        return pInt.area()\n",
        "\n",
        "    def compute_ap(confList, matchList, numGtCare):\n",
        "        correct = 0\n",
        "        AP = 0\n",
        "        if len(confList) > 0:\n",
        "            confList = np.array(confList)\n",
        "            matchList = np.array(matchList)\n",
        "            sorted_ind = np.argsort(-confList)\n",
        "            confList = confList[sorted_ind]\n",
        "            matchList = matchList[sorted_ind]\n",
        "            for n in range(len(confList)):\n",
        "                match = matchList[n]\n",
        "                if match:\n",
        "                    correct += 1\n",
        "                    AP += float(correct) / (n + 1)\n",
        "\n",
        "            if numGtCare > 0:\n",
        "                AP /= numGtCare\n",
        "\n",
        "        return AP\n",
        "\n",
        "    def transcription_match(\n",
        "        transGt,\n",
        "        transDet,\n",
        "        specialCharacters=str(r'!?.:,*\"()·[]/\\''),\n",
        "        onlyRemoveFirstLastCharacterGT=True,\n",
        "    ):\n",
        "\n",
        "        if onlyRemoveFirstLastCharacterGT:\n",
        "            # special characters in GT are allowed only at initial or final position\n",
        "            if transGt == transDet:\n",
        "                return True\n",
        "\n",
        "            if specialCharacters.find(transGt[0]) > -1:\n",
        "                if transGt[1:] == transDet:\n",
        "                    return True\n",
        "\n",
        "            if specialCharacters.find(transGt[-1]) > -1:\n",
        "                if transGt[0 : len(transGt) - 1] == transDet:\n",
        "                    return True\n",
        "\n",
        "            if (\n",
        "                specialCharacters.find(transGt[0]) > -1\n",
        "                and specialCharacters.find(transGt[-1]) > -1\n",
        "            ):\n",
        "                if transGt[1 : len(transGt) - 1] == transDet:\n",
        "                    return True\n",
        "            return False\n",
        "        else:\n",
        "            # Special characters are removed from the begining and the end of both Detection and GroundTruth\n",
        "            while len(transGt) > 0 and specialCharacters.find(transGt[0]) > -1:\n",
        "                transGt = transGt[1:]\n",
        "\n",
        "            while len(transDet) > 0 and specialCharacters.find(transDet[0]) > -1:\n",
        "                transDet = transDet[1:]\n",
        "\n",
        "            while len(transGt) > 0 and specialCharacters.find(transGt[-1]) > -1:\n",
        "                transGt = transGt[0 : len(transGt) - 1]\n",
        "\n",
        "            while len(transDet) > 0 and specialCharacters.find(transDet[-1]) > -1:\n",
        "                transDet = transDet[0 : len(transDet) - 1]\n",
        "\n",
        "            return transGt == transDet\n",
        "\n",
        "    def include_in_dictionary(transcription):\n",
        "        \"\"\"\n",
        "        Function used in Word Spotting that finds if the Ground Truth transcription meets the rules to enter into the dictionary. If not, the transcription will be cared as don't care\n",
        "        \"\"\"\n",
        "        # special case 's at final\n",
        "        if (\n",
        "            transcription[len(transcription) - 2 :] == \"'s\"\n",
        "            or transcription[len(transcription) - 2 :] == \"'S\"\n",
        "        ):\n",
        "            transcription = transcription[0 : len(transcription) - 2]\n",
        "\n",
        "        # hypens at init or final of the word\n",
        "        transcription = transcription.strip(\"-\")\n",
        "\n",
        "        specialCharacters = str(\"'!?.:,*\\\"()·[]/\")\n",
        "        # specialCharacters = str(\"`\")\n",
        "        for character in specialCharacters:\n",
        "            transcription = transcription.replace(character, \" \")\n",
        "\n",
        "        transcription = transcription.strip()\n",
        "\n",
        "        if len(transcription) != len(transcription.replace(\" \", \"\")):\n",
        "            # print(\"len(transcription.replace(\" \", \"\")): xx: \", transcription)\n",
        "            return False\n",
        "\n",
        "        # if len(transcription) < evaluationParams[\"MIN_LENGTH_CARE_WORD\"]:\n",
        "            # print(\"evaluati] xxxx: \", evaluationParams[\"MIN_LENGTH_CARE_WORD\"])\n",
        "            # print(\"evaluationParams xx: \", transcription)\n",
        "            # return False\n",
        "\n",
        "        notAllowed = str(\"×÷·\")\n",
        "\n",
        "        range1 = [ord(\"a\"), ord(\"z\")]\n",
        "        range2 = [ord(\"A\"), ord(\"Z\")]\n",
        "        range3 = [ord(\"À\"), ord(\"ƿ\")]\n",
        "        range4 = [ord(\"Ǆ\"), ord(\"ɿ\")]\n",
        "        range5 = [ord(\"Ά\"), ord(\"Ͽ\")]\n",
        "        range6 = [ord(\"-\"), ord(\"-\")]\n",
        "        range7 = [ord(\"Ạ\"), ord(\"Ỹ\")]\n",
        "        range8 = [ord(\"0\"), ord(\"9\")]\n",
        "\n",
        "        for char in transcription:\n",
        "            charCode = ord(char)\n",
        "            if notAllowed.find(char) != -1:\n",
        "                #c print(\"notAllowed xx: \", transcription)\n",
        "                return False\n",
        "\n",
        "            valid = (\n",
        "                (charCode >= range1[0] and charCode <= range1[1])\n",
        "                or (charCode >= range2[0] and charCode <= range2[1])\n",
        "                or (charCode >= range3[0] and charCode <= range3[1])\n",
        "                or (charCode >= range4[0] and charCode <= range4[1])\n",
        "                or (charCode >= range5[0] and charCode <= range5[1])\n",
        "                or (charCode >= range6[0] and charCode <= range6[1])\n",
        "                or (charCode >= range7[0] and charCode <= range7[1])\n",
        "                or (charCode >= range8[0] and charCode <= range8[1])\n",
        "            )\n",
        "            if valid == False:\n",
        "                #c print(\"transcription xx: \", transcription)\n",
        "                return False\n",
        "\n",
        "        # print(\"include_in_dictionary: \", transcription)\n",
        "        # print(\"include_in_dictionary 2: \", valid)\n",
        "        return True\n",
        "\n",
        "    def include_in_dictionary_transcription(transcription):\n",
        "        \"\"\"\n",
        "        Function applied to the Ground Truth transcriptions used in Word Spotting. It removes special characters or terminations\n",
        "        \"\"\"\n",
        "        # special case 's at final\n",
        "        if (\n",
        "            transcription[len(transcription) - 2 :] == \"'s\"\n",
        "            or transcription[len(transcription) - 2 :] == \"'S\"\n",
        "        ):\n",
        "            transcription = transcription[0 : len(transcription) - 2]\n",
        "\n",
        "        # hypens at init or final of the word\n",
        "        transcription = transcription.strip(\"-\")\n",
        "\n",
        "        specialCharacters = str(\"'!?.:,*\\\"()·[]/\")\\\n",
        "        # specialCharacters = str(\"`\")\n",
        "        for character in specialCharacters:\n",
        "            transcription = transcription.replace(character, \" \")\n",
        "\n",
        "        transcription = transcription.strip()\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    perSampleMetrics = {}\n",
        "\n",
        "    matchedSum = 0\n",
        "    det_only_matchedSum = 0\n",
        "\n",
        "    Rectangle = namedtuple(\"Rectangle\", \"xmin ymin xmax ymax\")\n",
        "\n",
        "    gt = load_zip_file(gtFilePath, evaluationParams[\"GT_SAMPLE_NAME_2_ID\"])\n",
        "    subm = load_zip_file(submFilePath, evaluationParams[\"DET_SAMPLE_NAME_2_ID\"], True)\n",
        "    print(\"submission file path: \", submFilePath)\n",
        "    print(\"gt file path: \", gtFilePath)\n",
        "    numGlobalCareGt = 0\n",
        "    numGlobalCareDet = 0\n",
        "    det_only_numGlobalCareGt = 0\n",
        "    det_only_numGlobalCareDet = 0\n",
        "\n",
        "    arrGlobalConfidences = []\n",
        "    arrGlobalMatches = []\n",
        "\n",
        "    x = 0\n",
        "    y = 0\n",
        "    max_fail = 0\n",
        "    name_fail = \"\"\n",
        "    for resFile in gt:\n",
        "        # print(resFile)\n",
        "        a = 0\n",
        "        b = 0\n",
        "        # print('resgt', resFile)\n",
        "        gtFile = decode_utf8(gt[resFile])\n",
        "        if gtFile is None:\n",
        "            raise Exception(\"The file %s is not UTF-8\" % resFile)\n",
        "\n",
        "        recall = 0\n",
        "        precision = 0\n",
        "        hmean = 0\n",
        "        detCorrect = 0\n",
        "        detOnlyCorrect = 0\n",
        "        iouMat = np.empty([1, 1])\n",
        "        gtPols = []\n",
        "        detPols = []\n",
        "        gtTrans = []\n",
        "        detTrans = []\n",
        "        gtPolPoints = []\n",
        "        detPolPoints = []\n",
        "        gtDontCarePolsNum = (\n",
        "            []\n",
        "        )  # Array of Ground Truth Polygons' keys marked as don't Care\n",
        "        det_only_gtDontCarePolsNum = []\n",
        "        detDontCarePolsNum = (\n",
        "            []\n",
        "        )  # Array of Detected Polygons' matched with a don't Care GT\n",
        "        det_only_detDontCarePolsNum = []\n",
        "        detMatchedNums = []\n",
        "        pairs = []\n",
        "\n",
        "        arrSampleConfidences = []\n",
        "        arrSampleMatch = []\n",
        "        sampleAP = 0\n",
        "\n",
        "        (\n",
        "            pointsList,\n",
        "            _,\n",
        "            transcriptionsList,\n",
        "        ) = get_tl_line_values_from_file_contents(\n",
        "            gtFile, evaluationParams[\"CRLF\"], evaluationParams[\"LTRB\"], True, False\n",
        "        )\n",
        "        # print(\"pointsList a: \", len(pointsList))\n",
        "        # print(\"transcriptionsList a: \", len(transcriptionsList))\n",
        "\n",
        "        for n in range(len(pointsList)):\n",
        "            points = pointsList[n]\n",
        "            transcription = transcriptionsList[n]\n",
        "            # print(\"transcriptionsList[n]: \", transcriptionsList[n])\n",
        "            det_only_dontCare = dontCare = (\n",
        "                transcription == \"###\"\n",
        "            )  # ctw1500 and total_text gt have been modified to the same format.\n",
        "            # print(\"dc 2: \", dontCare)\n",
        "            if evaluationParams[\"LTRB\"]:\n",
        "                gtRect = Rectangle(*points)\n",
        "                gtPol = rectangle_to_polygon(gtRect)\n",
        "            else:\n",
        "                gtPol = polygon_from_points(points)\n",
        "            gtPols.append(gtPol)\n",
        "            gtPolPoints.append(points)\n",
        "\n",
        "            # On word spotting we will filter some transcriptions with special characters\n",
        "            # print(\"dc 2.5: \", dontCare)\n",
        "            if evaluationParams[\"WORD_SPOTTING\"]:\n",
        "                if dontCare == False:\n",
        "                    if include_in_dictionary(transcription) == False:\n",
        "                        # if(resFile == '0001541'):\n",
        "                        #   print(\"dc 3: \", transcription)\n",
        "                        dontCare = True\n",
        "                    else:\n",
        "                        transcription = include_in_dictionary_transcription(\n",
        "                            transcription\n",
        "                        )\n",
        "\n",
        "            gtTrans.append(transcription)\n",
        "            # print(\"dontCare 2.9: \", dontCare)\n",
        "            if dontCare:\n",
        "                gtDontCarePolsNum.append(len(gtPols) - 1)\n",
        "                # print(\"dontCare 3: \", transcriptionsList[len(gtPols) - 1])\n",
        "            if det_only_dontCare:\n",
        "                # print(\"det_only_dontCare 3: \", transcriptionsList[len(gtPols) - 1])\n",
        "                det_only_gtDontCarePolsNum.append(len(gtPols) - 1)\n",
        "\n",
        "        if resFile in subm:\n",
        "\n",
        "            detFile = decode_utf8(subm[resFile])\n",
        "\n",
        "            (\n",
        "                pointsList,\n",
        "                confidencesList,\n",
        "                transcriptionsList,\n",
        "            ) = get_tl_line_values_from_file_contents_det(\n",
        "                detFile,\n",
        "                evaluationParams[\"CRLF\"],\n",
        "                evaluationParams[\"LTRB\"],\n",
        "                True,\n",
        "                evaluationParams[\"CONFIDENCES\"],\n",
        "            )\n",
        "\n",
        "            # print(\"transcriptionsList if resFile in subm:: \", transcriptionsList)\n",
        "            for n in range(len(pointsList)):\n",
        "                points = pointsList[n]\n",
        "                transcription = transcriptionsList[n]\n",
        "                # print(\"transcriptionsList[99]: \", transcriptionsList[n])\n",
        "\n",
        "                if evaluationParams[\"LTRB\"]:\n",
        "                    detRect = Rectangle(*points)\n",
        "                    detPol = rectangle_to_polygon(detRect)\n",
        "                else:\n",
        "                    detPol = polygon_from_points(points)\n",
        "                detPols.append(detPol)\n",
        "                detPolPoints.append(points)\n",
        "                transcription = include_in_dictionary_transcription(transcription)\n",
        "                detTrans.append(transcription)\n",
        "\n",
        "                if len(gtDontCarePolsNum) > 0:\n",
        "                    for dontCarePol in gtDontCarePolsNum:\n",
        "                        i = dontCarePol\n",
        "                        dontCarePol = gtPols[dontCarePol]\n",
        "                        intersected_area = get_intersection(dontCarePol, detPol)\n",
        "                        pdDimensions = detPol.area()\n",
        "                        precision = (\n",
        "                            0 if pdDimensions == 0 else intersected_area / pdDimensions\n",
        "                        )\n",
        "                        if precision > evaluationParams[\"AREA_PRECISION_CONSTRAINT\"]:\n",
        "                            # print(\"gtTrans: \", gtTrans[i])\n",
        "                            # print(\"transcription bi trung: \", transcription)\n",
        "                            detDontCarePolsNum.append(len(detPols) - 1)\n",
        "                            break\n",
        "\n",
        "                if len(det_only_gtDontCarePolsNum) > 0:\n",
        "                    for dontCarePol in det_only_gtDontCarePolsNum:\n",
        "                        dontCarePol = gtPols[dontCarePol]\n",
        "                        intersected_area = get_intersection(dontCarePol, detPol)\n",
        "                        pdDimensions = detPol.area()\n",
        "                        precision = (\n",
        "                            0 if pdDimensions == 0 else intersected_area / pdDimensions\n",
        "                        )\n",
        "                        if precision > evaluationParams[\"AREA_PRECISION_CONSTRAINT\"]:\n",
        "                            det_only_detDontCarePolsNum.append(len(detPols) - 1)\n",
        "                            break\n",
        "\n",
        "            if len(gtPols) > 0 and len(detPols) > 0:\n",
        "                # Calculate IoU and precision matrixs\n",
        "                outputShape = [len(gtPols), len(detPols)]\n",
        "                iouMat = np.empty(outputShape)\n",
        "                gtRectMat = np.zeros(len(gtPols), np.int8)\n",
        "                detRectMat = np.zeros(len(detPols), np.int8)\n",
        "                det_only_gtRectMat = np.zeros(len(gtPols), np.int8)\n",
        "                det_only_detRectMat = np.zeros(len(detPols), np.int8)\n",
        "                for gtNum in range(len(gtPols)):\n",
        "                    for detNum in range(len(detPols)):\n",
        "                        pG = gtPols[gtNum]\n",
        "                        pD = detPols[detNum]\n",
        "                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n",
        "\n",
        "                        # print(\"iou: \", iouMat[gtNum, detNum])\n",
        "\n",
        "                for gtNum in range(len(gtPols)):\n",
        "                    for detNum in range(len(detPols)):\n",
        "                        if (\n",
        "                            gtRectMat[gtNum] == 0\n",
        "                            and detRectMat[detNum] == 0\n",
        "                            and gtNum not in gtDontCarePolsNum\n",
        "                            and detNum not in detDontCarePolsNum\n",
        "                        ):\n",
        "                            if (\n",
        "                                iouMat[gtNum, detNum]\n",
        "                                > evaluationParams[\"IOU_CONSTRAINT\"]\n",
        "                            ):\n",
        "                                gtRectMat[gtNum] = 1\n",
        "                                detRectMat[detNum] = 1\n",
        "                                # detection matched only if transcription is equal\n",
        "                                # det_only_correct = True\n",
        "                                # detOnlyCorrect += 1\n",
        "                                if evaluationParams[\"WORD_SPOTTING\"]:\n",
        "                                    edd = lstn.distance(\n",
        "                                        gtTrans[gtNum].upper(), detTrans[detNum].upper()\n",
        "                                    )\n",
        "                                    # if(resFile == '0001541'):\n",
        "                                    #   print(gtTrans[gtNum].upper(), detTrans[detNum].upper())\n",
        "                                    if edd <= 0:\n",
        "                                        correct = True\n",
        "                                        x += 1\n",
        "                                        a += 1\n",
        "                                    else:\n",
        "                                        correct = False\n",
        "                                        y += 1\n",
        "                                        b += 1\n",
        "\n",
        "                                    # print(detNum, \" \", gtTrans[gtNum], \" \", detTrans[detNum], \" \", correct)\n",
        "                                    # correct = gtTrans[gtNum].upper() == detTrans[detNum].upper()\n",
        "                                else:\n",
        "                                    try:\n",
        "                                        correct = (\n",
        "                                            transcription_match(\n",
        "                                                gtTrans[gtNum].upper(),\n",
        "                                                detTrans[detNum].upper(),\n",
        "                                                evaluationParams[\"SPECIAL_CHARACTERS\"],\n",
        "                                                evaluationParams[\n",
        "                                                    \"ONLY_REMOVE_FIRST_LAST_CHARACTER\"\n",
        "                                                ],\n",
        "                                            )\n",
        "                                            == True\n",
        "                                        )\n",
        "                                    except:  # empty\n",
        "                                        correct = False\n",
        "                                detCorrect += 1 if correct else 0\n",
        "                                if correct:\n",
        "                                    detMatchedNums.append(detNum)\n",
        "\n",
        "                for gtNum in range(len(gtPols)):\n",
        "                    for detNum in range(len(detPols)):\n",
        "                        if (\n",
        "                            det_only_gtRectMat[gtNum] == 0\n",
        "                            and det_only_detRectMat[detNum] == 0\n",
        "                            and gtNum not in det_only_gtDontCarePolsNum\n",
        "                            and detNum not in det_only_detDontCarePolsNum\n",
        "                        ):\n",
        "                            if (\n",
        "                                iouMat[gtNum, detNum]\n",
        "                                > evaluationParams[\"IOU_CONSTRAINT\"]\n",
        "                            ):\n",
        "                                det_only_gtRectMat[gtNum] = 1\n",
        "                                det_only_detRectMat[detNum] = 1\n",
        "                                # detection matched only if transcription is equal\n",
        "                                det_only_correct = True\n",
        "                                detOnlyCorrect += 1\n",
        "\n",
        "        numGtCare = len(gtPols) - len(gtDontCarePolsNum)\n",
        "        numDetCare = len(detPols) - len(detDontCarePolsNum)\n",
        "        # print(\"numDetCare: \", numDetCare)\n",
        "        # print(\"len(detPols): \", len(detPols))\n",
        "        # print(\"len(detDontCarePolsNum): \", len(detDontCarePolsNum))\n",
        "        # print(\"a: \", a)\n",
        "        # print(\"b: \", b)\n",
        "        # print(\"a + b: \", a+b)\n",
        "        # if(numGtCare != a + b):\n",
        "          # print(resFile)\n",
        "          # print(gtPols)\n",
        "          # print(gtDontCarePolsNum)\n",
        "\n",
        "        fail = (numDetCare - a - b)\n",
        "        if (fail > max_fail and (resFile != '0001774' and resFile != '0001509' and resFile != '0001755'\n",
        "                                 and resFile != '0001560'\n",
        "                                 and resFile != '0001716'\n",
        "                                 and resFile != '0001772'\n",
        "                                 and resFile != '0001634'\n",
        "                                 and resFile != '0001833'\n",
        "                                 and resFile != '0001554'\n",
        "                                  and resFile != '0001570'\n",
        "                                and resFile != '0001713'\n",
        "                                and resFile != '0001585'\n",
        "                                and resFile != '0001508'\n",
        "                                and resFile != '0001662'\n",
        "                                and resFile != '0001738'\n",
        "                                and resFile != '0001848'\n",
        "                                and resFile != '0001541'\n",
        "                                and resFile != '0001603'\n",
        "                                and resFile != '0001836'\n",
        "                                and resFile != '0001725'\n",
        "                                and resFile != '0001676'\n",
        "                                and resFile != '0001778'\n",
        "                                and resFile != '0001640'\n",
        "                                and resFile != '0001990'\n",
        "                                and resFile != '0001995'\n",
        "                                and resFile != '0001623'\n",
        "                                and resFile != '0001751'\n",
        "                                and resFile != '0001999'\n",
        "                                and resFile != '0001517'\n",
        "                                and resFile != '0001528'\n",
        "                                and resFile != '0001531'\n",
        "                                and resFile != '0001639')):\n",
        "          max_fail = fail\n",
        "          name_fail = resFile\n",
        "\n",
        "        det_only_numGtCare = len(gtPols) - len(det_only_gtDontCarePolsNum)\n",
        "        det_only_numDetCare = len(detPols) - len(det_only_detDontCarePolsNum)\n",
        "        if numGtCare == 0:\n",
        "            recall = float(1)\n",
        "            precision = float(0) if numDetCare > 0 else float(1)\n",
        "        else:\n",
        "            recall = float(detCorrect) / numGtCare\n",
        "            precision = 0 if numDetCare == 0 else float(detCorrect) / numDetCare\n",
        "\n",
        "        if det_only_numGtCare == 0:\n",
        "            det_only_recall = float(1)\n",
        "            det_only_precision = float(0) if det_only_numDetCare > 0 else float(1)\n",
        "        else:\n",
        "            det_only_recall = float(detOnlyCorrect) / det_only_numGtCare\n",
        "            det_only_precision = (\n",
        "                0\n",
        "                if det_only_numDetCare == 0\n",
        "                else float(detOnlyCorrect) / det_only_numDetCare\n",
        "            )\n",
        "\n",
        "        hmean = (\n",
        "            0\n",
        "            if (precision + recall) == 0\n",
        "            else 2.0 * precision * recall / (precision + recall)\n",
        "        )\n",
        "        det_only_hmean = (\n",
        "            0\n",
        "            if (det_only_precision + det_only_recall) == 0\n",
        "            else 2.0\n",
        "            * det_only_precision\n",
        "            * det_only_recall\n",
        "            / (det_only_precision + det_only_recall)\n",
        "        )\n",
        "\n",
        "        matchedSum += detCorrect\n",
        "        det_only_matchedSum += detOnlyCorrect\n",
        "        numGlobalCareGt += numGtCare\n",
        "        numGlobalCareDet += numDetCare\n",
        "        det_only_numGlobalCareGt += det_only_numGtCare\n",
        "        det_only_numGlobalCareDet += det_only_numDetCare\n",
        "\n",
        "        perSampleMetrics[resFile] = {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"hmean\": hmean,\n",
        "            \"iouMat\": [] if len(detPols) > 100 else iouMat.tolist(),\n",
        "            \"gtPolPoints\": gtPolPoints,\n",
        "            \"detPolPoints\": detPolPoints,\n",
        "            \"gtTrans\": gtTrans,\n",
        "            \"detTrans\": detTrans,\n",
        "            \"gtDontCare\": gtDontCarePolsNum,\n",
        "            \"detDontCare\": detDontCarePolsNum,\n",
        "            \"evaluationParams\": evaluationParams,\n",
        "        }\n",
        "\n",
        "    print(\"X: \", x)\n",
        "    print(\"Y: \", y)\n",
        "    print(\"X+Y: \", x+y)\n",
        "    print(\"matchedSum: \", matchedSum)\n",
        "    print(\"numGlobalCareDet: \", numGlobalCareDet)\n",
        "    print(\"max_fail: \", max_fail)\n",
        "    print(\"name_fail: \", name_fail)\n",
        "    print(\"-------\")\n",
        "    print(\"numGlobalCareGt: \", numGlobalCareGt)\n",
        "    methodRecall = 0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n",
        "    methodPrecision = (\n",
        "        0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n",
        "    )\n",
        "    methodHmean = (\n",
        "        0\n",
        "        if methodRecall + methodPrecision == 0\n",
        "        else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n",
        "    )\n",
        "\n",
        "    det_only_methodRecall = (\n",
        "        0\n",
        "        if det_only_numGlobalCareGt == 0\n",
        "        else float(det_only_matchedSum) / det_only_numGlobalCareGt\n",
        "    )\n",
        "    det_only_methodPrecision = (\n",
        "        0\n",
        "        if det_only_numGlobalCareDet == 0\n",
        "        else float(det_only_matchedSum) / det_only_numGlobalCareDet\n",
        "    )\n",
        "    det_only_methodHmean = (\n",
        "        0\n",
        "        if det_only_methodRecall + det_only_methodPrecision == 0\n",
        "        else 2\n",
        "        * det_only_methodRecall\n",
        "        * det_only_methodPrecision\n",
        "        / (det_only_methodRecall + det_only_methodPrecision)\n",
        "    )\n",
        "\n",
        "    methodMetrics = r\"E2E_RESULTS: precision: {}, recall: {}, hmean: {}\".format(\n",
        "        methodPrecision, methodRecall, methodHmean\n",
        "    )\n",
        "    det_only_methodMetrics = (\n",
        "        r\"DETECTION_ONLY_RESULTS: precision: {}, recall: {}, hmean: {}\".format(\n",
        "            det_only_methodPrecision, det_only_methodRecall, det_only_methodHmean\n",
        "        )\n",
        "    )\n",
        "\n",
        "    resDict = {\n",
        "        \"calculated\": True,\n",
        "        \"Message\": \"\",\n",
        "        \"e2e_method\": methodMetrics,\n",
        "        \"det_only_method\": det_only_methodMetrics,\n",
        "        \"per_sample\": perSampleMetrics,\n",
        "    }\n",
        "\n",
        "    return resDict\n",
        "\n",
        "\n",
        "def load_zip_file(file, fileNameRegExp=\"\", allEntries=False):\n",
        "    \"\"\"\n",
        "    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n",
        "    The key's are the names or the file or the capturing group definied in the fileNameRegExp\n",
        "    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n",
        "    \"\"\"\n",
        "    try:\n",
        "        archive = zipfile.ZipFile(file, mode=\"r\", allowZip64=True)\n",
        "    except:\n",
        "        raise Exception(\"Error loading the ZIP archive\")\n",
        "\n",
        "    pairs = []\n",
        "    for name in archive.namelist():\n",
        "        addFile = True\n",
        "        keyName = name\n",
        "        if fileNameRegExp != \"\":\n",
        "            m = re.match(fileNameRegExp, name)\n",
        "            if m == None:\n",
        "                addFile = False\n",
        "            else:\n",
        "                if len(m.groups()) > 0:\n",
        "                    keyName = m.group(1)\n",
        "\n",
        "        if addFile:\n",
        "            pairs.append([keyName, archive.read(name)])\n",
        "        else:\n",
        "            if allEntries:\n",
        "                raise Exception(\"ZIP entry not valid: %s\" % name)\n",
        "\n",
        "    return dict(pairs)\n",
        "\n",
        "\n",
        "def decode_utf8(raw):\n",
        "    \"\"\"\n",
        "    Returns a Unicode object on success, or None on failure\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw = codecs.decode(raw, \"utf-8\", \"replace\")\n",
        "        # extracts BOM if exists\n",
        "        raw = raw.encode(\"utf8\")\n",
        "        if raw.startswith(codecs.BOM_UTF8):\n",
        "            raw = raw.replace(codecs.BOM_UTF8, \"\", 1)\n",
        "        return raw.decode(\"utf-8\")\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_tl_line_values_from_file_contents(\n",
        "    content,\n",
        "    CRLF=True,\n",
        "    LTRB=True,\n",
        "    withTranscription=False,\n",
        "    withConfidence=False,\n",
        "    imWidth=0,\n",
        "    imHeight=0,\n",
        "    sort_by_confidences=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n",
        "    xmin,ymin,xmax,ymax,[confidence],[transcription]\n",
        "    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n",
        "    \"\"\"\n",
        "    pointsList = []\n",
        "    transcriptionsList = []\n",
        "    confidencesList = []\n",
        "\n",
        "    lines = content.split(\"\\r\\n\" if CRLF else \"\\n\")\n",
        "    for line in lines:\n",
        "        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "        if line != \"\":\n",
        "            points, confidence, transcription = get_tl_line_values_gt(\n",
        "                line, LTRB, withTranscription, withConfidence, imWidth, imHeight\n",
        "            )\n",
        "            pointsList.append(points)\n",
        "            transcriptionsList.append(transcription)\n",
        "            confidencesList.append(confidence)\n",
        "\n",
        "    # print(\"NOTE 4: \", transcriptionsList)\n",
        "    # print(\"len(tran)\", len(transcriptionsList))\n",
        "    if withConfidence and len(confidencesList) > 0 and sort_by_confidences:\n",
        "        import numpy as np\n",
        "\n",
        "        sorted_ind = np.argsort(-np.array(confidencesList))\n",
        "        confidencesList = [confidencesList[i] for i in sorted_ind]\n",
        "        pointsList = [pointsList[i] for i in sorted_ind]\n",
        "        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]\n",
        "\n",
        "    # print(\"NOTE 5: \", transcriptionsList)\n",
        "    return pointsList, confidencesList, transcriptionsList\n",
        "\n",
        "\n",
        "def get_tl_line_values_from_file_contents_det(\n",
        "    content,\n",
        "    CRLF=True,\n",
        "    LTRB=True,\n",
        "    withTranscription=False,\n",
        "    withConfidence=False,\n",
        "    imWidth=0,\n",
        "    imHeight=0,\n",
        "    sort_by_confidences=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n",
        "    xmin,ymin,xmax,ymax,[confidence],[transcription]\n",
        "    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n",
        "    \"\"\"\n",
        "    pointsList = []\n",
        "    transcriptionsList = []\n",
        "    confidencesList = []\n",
        "\n",
        "    lines = content.split(\"\\r\\n\" if CRLF else \"\\n\")\n",
        "    for line in lines:\n",
        "        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "        if line != \"\":\n",
        "            points, confidence, transcription = get_tl_line_values(\n",
        "                line, LTRB, withTranscription, withConfidence, imWidth, imHeight\n",
        "            )\n",
        "            pointsList.append(points)\n",
        "            transcriptionsList.append(transcription)\n",
        "            confidencesList.append(confidence)\n",
        "\n",
        "    if withConfidence and len(confidencesList) > 0 and sort_by_confidences:\n",
        "        import numpy as np\n",
        "\n",
        "        sorted_ind = np.argsort(-np.array(confidencesList))\n",
        "        confidencesList = [confidencesList[i] for i in sorted_ind]\n",
        "        pointsList = [pointsList[i] for i in sorted_ind]\n",
        "        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]\n",
        "\n",
        "    return pointsList, confidencesList, transcriptionsList\n",
        "\n",
        "\n",
        "def get_tl_line_values_gt(\n",
        "    line,\n",
        "    LTRB=True,\n",
        "    withTranscription=False,\n",
        "    withConfidence=False,\n",
        "    imWidth=0,\n",
        "    imHeight=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Validate the format of the line. If the line is not valid an exception will be raised.\n",
        "    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n",
        "    Posible values are:\n",
        "    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription]\n",
        "    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription]\n",
        "    Returns values from a textline. Points , [Confidences], [Transcriptions]\n",
        "    \"\"\"\n",
        "    confidence = 0.0\n",
        "    transcription = \"\"\n",
        "    points = []\n",
        "\n",
        "    if LTRB:\n",
        "        # do not use\n",
        "        raise Exception(\"Not implemented.\")\n",
        "\n",
        "    else:\n",
        "        # if withTranscription and withConfidence:\n",
        "        #     cors = line.split(',')\n",
        "        #     assert(len(cors)%2 -2 == 0), 'num cors should be even.'\n",
        "        #     try:\n",
        "        #         points = [ float(ic) for ic in cors[:-2]]\n",
        "        #     except Exception as e:\n",
        "        #         raise(e)\n",
        "        # elif withConfidence:\n",
        "        #     cors = line.split(',')\n",
        "        #     assert(len(cors)%2 -1 == 0), 'num cors should be even.'\n",
        "        #     try:\n",
        "        #         points = [ float(ic) for ic in cors[:-1]]\n",
        "        #     except Exception as e:\n",
        "        #         raise(e)\n",
        "        # elif withTranscription:\n",
        "        #     cors = line.split(',')\n",
        "        #     assert(len(cors)%2 -1 == 0), 'num cors should be even.'\n",
        "        #     try:\n",
        "        #         points = [ float(ic) for ic in cors[:-1]]\n",
        "        #     except Exception as e:\n",
        "        #         raise(e)\n",
        "        # else:\n",
        "        #     cors = line.split(',')\n",
        "        #     assert(len(cors)%2 == 0), 'num cors should be even.'\n",
        "        #     try:\n",
        "        #         points = [ float(ic) for ic in cors[:]]\n",
        "        #     except Exception as e:\n",
        "        #         raise(e)\n",
        "\n",
        "        if withTranscription and withConfidence:\n",
        "            raise (\"not implemented\")\n",
        "        elif withConfidence:\n",
        "            raise (\"not implemented\")\n",
        "        elif withTranscription:\n",
        "            ptr = line.strip().split(\",####\")\n",
        "            # print(\"line \", line)\n",
        "            cors = ptr[0].split(\",\")\n",
        "            recs = ptr[1].strip()\n",
        "            assert len(cors) % 2 == 0, \"num cors should be even.\"\n",
        "            try:\n",
        "                points = [float(ic) for ic in cors[:]]\n",
        "            except Exception as e:\n",
        "                raise (e)\n",
        "        else:\n",
        "            raise (\"not implemented\")\n",
        "\n",
        "        #        validate_clockwise_points(points)\n",
        "\n",
        "        if imWidth > 0 and imHeight > 0:\n",
        "            for ip in range(0, len(points), 2):\n",
        "                validate_point_inside_bounds(\n",
        "                    points[ip], points[ip + 1], imWidth, imHeight\n",
        "                )\n",
        "\n",
        "    if withConfidence:\n",
        "        try:\n",
        "            confidence = 1.0\n",
        "        except ValueError:\n",
        "            raise Exception(\"Confidence value must be a float\")\n",
        "\n",
        "    if withTranscription:\n",
        "        # posTranscription = numPoints + (2 if withConfidence else 1)\n",
        "        # transcription = cors[-1].strip()\n",
        "        transcription = recs\n",
        "        m2 = re.match(r\"^\\s*\\\"(.*)\\\"\\s*$\", transcription)\n",
        "        if (\n",
        "            m2 != None\n",
        "        ):  # Transcription with double quotes, we extract the value and replace escaped characters\n",
        "            transcription = m2.group(1).replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n",
        "\n",
        "    return points, confidence, transcription\n",
        "\n",
        "\n",
        "def get_tl_line_values(\n",
        "    line,\n",
        "    LTRB=True,\n",
        "    withTranscription=False,\n",
        "    withConfidence=False,\n",
        "    imWidth=0,\n",
        "    imHeight=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Validate the format of the line. If the line is not valid an exception will be raised.\n",
        "    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n",
        "    Posible values are:\n",
        "    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription]\n",
        "    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription]\n",
        "    Returns values from a textline. Points , [Confidences], [Transcriptions]\n",
        "    \"\"\"\n",
        "    confidence = 0.0\n",
        "    transcription = \"\"\n",
        "    points = []\n",
        "\n",
        "    # print(\"withConfidence: \", withConfidence)\n",
        "\n",
        "    if LTRB:\n",
        "        # do not use\n",
        "        raise Exception(\"Not implemented.\")\n",
        "\n",
        "    else:\n",
        "        if withTranscription and withConfidence:\n",
        "            raise (\"not implemented\")\n",
        "        elif withConfidence:\n",
        "            raise (\"not implemented\")\n",
        "        elif withTranscription:\n",
        "            ptr = line.strip().split(\",####\")\n",
        "            cors = ptr[0].split(\",\")\n",
        "            recs = ptr[1].strip()\n",
        "            assert len(cors) % 2 == 0, \"num cors should be even.\"\n",
        "            try:\n",
        "                points = [float(ic) for ic in cors[:]]\n",
        "            except Exception as e:\n",
        "                raise (e)\n",
        "        else:\n",
        "            raise (\"not implemented\")\n",
        "\n",
        "        # print('det clock wise')\n",
        "        #        validate_clockwise_points(points)\n",
        "\n",
        "        if imWidth > 0 and imHeight > 0:\n",
        "            for ip in range(0, len(points), 2):\n",
        "                validate_point_inside_bounds(\n",
        "                    points[ip], points[ip + 1], imWidth, imHeight\n",
        "                )\n",
        "\n",
        "    if withConfidence:\n",
        "        try:\n",
        "            confidence = 1.0\n",
        "        except ValueError:\n",
        "            raise Exception(\"Confidence value must be a float\")\n",
        "\n",
        "    if withTranscription:\n",
        "        # posTranscription = numPoints + (2 if withConfidence else 1)\n",
        "        transcription = recs\n",
        "        # print(\"NOTE 1: \", transcription)\n",
        "        m2 = re.match(r\"^\\s*\\\"(.*)\\\"\\s*$\", transcription)\n",
        "        if (\n",
        "            m2 != None\n",
        "        ):  # Transcription with double quotes, we extract the value and replace escaped characters\n",
        "            # print(\"NOTE 2: \", transcription)\n",
        "            transcription = m2.group(1).replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n",
        "\n",
        "    # print(\"NOTE 1: \", transcription)\n",
        "    return points, confidence, transcription\n",
        "\n",
        "\n",
        "def validate_point_inside_bounds(x, y, imWidth, imHeight):\n",
        "    if x < 0 or x > imWidth:\n",
        "        raise Exception(\n",
        "            \"X value (%s) not valid. Image dimensions: (%s,%s)\" % (x, imWidth, imHeight)\n",
        "        )\n",
        "    if y < 0 or y > imHeight:\n",
        "        raise Exception(\n",
        "            \"Y value (%s)  not valid. Image dimensions: (%s,%s) Sample: %s Line:%s\"\n",
        "            % (y, imWidth, imHeight)\n",
        "        )\n",
        "\n",
        "\n",
        "dir_to_val = \"./\"+ destination_dir + \".zip\"\n",
        "evaluate_with_official_code(dir_to_val, \"./vintext_label.zip\");\n",
        "# evaluate_with_official_code(\"./vintext_label.zip\", \"./vintext_label.zip\");\n",
        "print(dir_to_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD9o95-J3KEQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uhQRlTW_-1-"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6skdBllyjLvn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}